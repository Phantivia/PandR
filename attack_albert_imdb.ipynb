{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pakages imported\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "print(\"pakages imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MyBertSelfAttention_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers.models.albert.modeling_albert import AlbertLayerGroup\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "INIT_MAG = 1.6e-1\n",
    "MAX_NORM = 1000\n",
    "\n",
    "class MyAlbertTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.embedding_hidden_mapping_in = nn.Linear(config.embedding_size, config.hidden_size)\n",
    "        self.albert_layer_groups = nn.ModuleList([AlbertLayerGroup(config) for _ in range(config.num_hidden_groups)])\n",
    "        self.perturb = None\n",
    "        self.perturb_locked = False\n",
    "        self.perturb_pos = -1\n",
    "        \n",
    "    #modified\n",
    "    \n",
    "    def p_init(self, input_length):\n",
    "        SQRT_NUMEL = (input_length * self.config.hidden_size) ** 0.5\n",
    "        self.perturb = torch.zeros((1, input_length, self.config.hidden_size), device = torch.device('cuda')).uniform_(-INIT_MAG, INIT_MAG)/SQRT_NUMEL\n",
    "        self.perturb.requires_grad_()\n",
    "        \n",
    "    def p_accu(self, loss, adv_lr, input_length = 5):\n",
    "        grad = torch.autograd.grad(loss, self.perturb)[0]\n",
    "        grad = (adv_lr * grad/grad.norm()).detach()\n",
    "        \n",
    "        grad[:, 0, :] = 0.0\n",
    "        grad[:, -1, :] = 0.0\n",
    "        \n",
    "        self.perturb = (self.perturb + grad).detach()\n",
    "        n = self.perturb.norm()\n",
    "        if n > MAX_NORM:\n",
    "            self.perturb = (MAX_NORM * self.perturb/n).detach()\n",
    "        self.perturb.grad = None\n",
    "        self.perturb.requires_grad_()\n",
    "        \n",
    "    def set_pos(self, pos):\n",
    "        self.perturb_pos = pos\n",
    "    \n",
    "    def p_lock(self):\n",
    "        self.perturb_locked = True\n",
    "    def p_unlock(self):\n",
    "        self.perturb_locked = False\n",
    "    \n",
    "    ##here\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        \n",
    "        hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n",
    "\n",
    "        all_hidden_states = (hidden_states,) if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        for i in range(self.config.num_hidden_layers):\n",
    "            # Number of layers in a hidden group\n",
    "            if i == self.perturb_pos and self.perturb is not None and not self.perturb_locked:\n",
    "                hidden_states = hidden_states + self.perturb\n",
    "            layers_per_group = int(self.config.num_hidden_layers / self.config.num_hidden_groups)\n",
    "\n",
    "            # Index of the hidden group\n",
    "            group_idx = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n",
    "\n",
    "            layer_group_output = self.albert_layer_groups[group_idx](\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                head_mask[group_idx * layers_per_group : (group_idx + 1) * layers_per_group],\n",
    "                output_attentions,\n",
    "                output_hidden_states,\n",
    "            )\n",
    "            hidden_states = layer_group_output[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + layer_group_output[-1]\n",
    "\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "task = \"imdb\"\n",
    "model_checkpoint = \"albert-base-v2\"\n",
    "model_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d43965853e4784a3bb1f78eb369d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d059e4a11547789d272115ca9c1a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import datasets\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "import math\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import datasets\n",
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = datasets.DatasetDict.load_from_disk(\"glue/\" + actual_task) if task != \"imdb\" else datasets.load_from_disk('imdb')\n",
    "\n",
    "if task == \"imdb\":\n",
    "    del dataset['unsupervised']\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "task_to_keys = {  \n",
    "            \"sst2\": (\"sentence\", None),\n",
    "            \"imdb\": (\"text\", None)\n",
    "        }\n",
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e8c5a7c267438da8296fe746ded8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff637bfaad24ffa9687a8d1cd3bebd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AlbertForSequenceClassification, AutoModelForSequenceClassification\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True, keep_in_memory = True)\n",
    "\n",
    "\n",
    "# model = AlbertForSequenceClassification.from_pretrained(model_checkpoint, num_labels = num_labels, mirror = \"bfsu\")\n",
    "model = torch.load(\"IMDB-albert-fullfinetuned-8848.pt\")\n",
    "torch.save(model.state_dict(),\"temp_state.pth\")\n",
    "\n",
    "model.albert.encoder = MyAlbertTransformer(model.config)\n",
    "    \n",
    "model.load_state_dict(torch.load(\"temp_state.pth\"))\n",
    "encoded_dataset.set_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer,AlbertForMaskedLM\n",
    "\n",
    "\n",
    "model_mlm = AlbertForMaskedLM.from_pretrained(model_checkpoint)\n",
    "probe_layer = model_mlm.predictions\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "victim_model = torch.load(\"IMDB-albert-fullfinetuned-8848.pt\")\n",
    "victim_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "class USE:\n",
    "    def __init__(self):\n",
    "        self.embed = hub.load(\"use\")\n",
    "\n",
    "    def count_use(self, sentence1, sentence2):\n",
    "        embeddings = self.embed([sentence1, sentence2])\n",
    "\n",
    "        vector1 = tf.reshape(embeddings[0], [512, 1])\n",
    "        vector2 = tf.reshape(embeddings[1], [512, 1])\n",
    "\n",
    "        return tf.matmul(vector1, vector2, transpose_a=True).numpy()[0][0]\n",
    "use = USE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<unk>', '[CLS]', '[SEP]', '[MASK]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "class Prober():\n",
    "    \n",
    "    def __init__(self, tokenizer, probe_layer, victim_model):\n",
    "        self.celoss = CrossEntropyLoss()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.probe_layer = probe_layer\n",
    "        self.victim = victim_model\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.probe_layer.cuda()\n",
    "            \n",
    "    def decode(self, hidden, origin_ids = None):\n",
    "        probe = self.probe_layer(hidden)\n",
    "        \n",
    "        _probe = probe.detach()\n",
    "        _probe[:, :,0:5] = 0.0\n",
    "        #mask token\n",
    "        \n",
    "        \n",
    "        reconstruction_ids = torch.topk(_probe, 1, -1)[1].squeeze(-1)\n",
    "        \n",
    "        reconstruction_ids[:, 0] = 2\n",
    "        reconstruction_ids[:,-1] = 3\n",
    "        \n",
    "        if origin_ids is None:\n",
    "            return reconstruction_ids\n",
    "        \n",
    "        origin_ids = origin_ids[:, 1:-1].long().cuda()\n",
    "        probe = probe[:, 1:-1, :]\n",
    "\n",
    "        \n",
    "        decode_loss = self.celoss(probe.reshape(-1, tokenizer.vocab_size), origin_ids.reshape(-1))\n",
    "\n",
    "        return decode_loss, reconstruction_ids\n",
    "        \n",
    "    \n",
    "    def attack(self, ids, answer_label):\n",
    "        \n",
    "        ids = ids.unsqueeze(0).cpu()\n",
    "        attention_mask = (torch.zeros(ids.shape) + 1).long().cpu()\n",
    "        answer_label = answer_label.cpu()\n",
    "        \n",
    "        p_layer = model.albert.encoder\n",
    "        \n",
    "        output = self.victim(input_ids = ids, attention_mask = attention_mask, labels = answer_label)\n",
    "        \n",
    "        logits = output[1]\n",
    "\n",
    "        adv_answer = logits.argmax(dim = 1)\n",
    "        \n",
    "        ids = ids[:, 1:-1]\n",
    "        reconstruction_tokens = self.tokenizer.batch_decode(ids)\n",
    "        reconstruction_sentence = self.tokenizer.convert_tokens_to_string(reconstruction_tokens)\n",
    "#         print(reconstruction_sentence)\n",
    "\n",
    "#         print(adv_answer,answer_label, \"LOGITS: \", logits )\n",
    "        reconstruction_sentence = None\n",
    "        \n",
    "        \n",
    "        if not adv_answer.equal(answer_label):\n",
    "            #print(\"CALL\")\n",
    "            reconstruction_tokens = self.tokenizer.batch_decode(ids)\n",
    "            reconstruction_sentence = self.tokenizer.convert_tokens_to_string(reconstruction_tokens)\n",
    "            \n",
    "        return reconstruction_sentence\n",
    "    \n",
    "prober = Prober(tokenizer, probe_layer, victim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def cal_metric(s1, s2):\n",
    "    _use = use.count_use(s1,s2)\n",
    "    return _use,0,0\n",
    "\n",
    "def attack_step(dataset = None, encoded_dataset = None, index = 0, \n",
    "                adv_lr = 3e-2, adv_steps = 3, start_layer = 0, prober = None, \n",
    "                scheduler = None, SEED = 114514):\n",
    "    \n",
    "    encoded_dataset.set_format(\"numpy\")\n",
    "    \n",
    "    def model_forward(model, input_ids, attention_mask, labels):\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask, labels = labels, output_hidden_states = True)\n",
    "        loss = output[0]\n",
    "        hidden = output[2][HIDDEN_INDEX]\n",
    "        logits = output[1]\n",
    "        return loss,logits,hidden\n",
    "    \n",
    "    def random_cover(ids):\n",
    "        import random \n",
    "        rindex = random.randint(1, input_length- 2)\n",
    "        ids[:, rindex] = 4\n",
    "        return ids\n",
    "    \n",
    "    \n",
    "\n",
    "    ori_sentence = dataset[key][index]['text']\n",
    "    ori_input_ids = torch.tensor(encoded_dataset[key]['input_ids'][index]).unsqueeze(0).cuda()\n",
    "    attention_mask = torch.tensor(encoded_dataset[key]['attention_mask'][index]).unsqueeze(0).cuda()\n",
    "    label = torch.tensor(encoded_dataset[key]['label'][index]).unsqueeze(0).cuda()\n",
    "    \n",
    "    #print(\"LABEL\", label)\n",
    "    correct_test = prober.attack(ori_input_ids[0], label)\n",
    "    #print(correct_test)\n",
    "    if correct_test is not None:\n",
    "        return None\n",
    "    \n",
    "    #BATCH SET UP\n",
    "    input_length = len(ori_input_ids[0])\n",
    "    input_ids = random_cover(ori_input_ids.clone())\n",
    "    \n",
    "    \n",
    "    p_layer = model.albert.encoder\n",
    "    p_layer.set_pos(start_layer)\n",
    "    p_layer.perturb = None\n",
    "    \n",
    "        \n",
    "    \n",
    "    successed = False\n",
    "    query = 0\n",
    "    pll = 0\n",
    "    I = 0\n",
    "    use = 0\n",
    "    bpstep = 0\n",
    "    rec_token_acc = 0\n",
    "    adv_sentence = None\n",
    "    \n",
    "    id_base = []\n",
    "    id_base.append(input_ids.tolist())\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    origin_loss, origin_logits, origin_hidden = model_forward(model, input_ids=input_ids, attention_mask=attention_mask, labels = label)\n",
    "    reconstruct_ids = prober.decode(origin_hidden)\n",
    "\n",
    "    rec_token_acc = ((input_ids == reconstruct_ids).sum() / input_ids.numel()).tolist()\n",
    "    \n",
    "    for i in reconstruct_ids:\n",
    "        if not i.tolist() in id_base:\n",
    "            id_base.append(i.tolist())\n",
    "            query += 1\n",
    "            adv_sentence = prober.attack(i, label)\n",
    "            if adv_sentence is not None:\n",
    "                use,I,ppl = cal_metric(ori_sentence, adv_sentence)\n",
    "                #print(\"USE = \", use)\n",
    "                if use > USE_GATE:\n",
    "                    successed = True\n",
    "                    bpstep = -1\n",
    "                    successed = True\n",
    "                    return{\n",
    "                        'successed': successed,\n",
    "                        'query': query,\n",
    "                        'pll': pll,\n",
    "                        'I': I,\n",
    "                        'use': use,\n",
    "                        'bpstep': bpstep,\n",
    "                        'token_acc': rec_token_acc,\n",
    "                        'ori_sentence': ori_sentence,\n",
    "                        'adv_sentence': adv_sentence\n",
    "                    }\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    p_layer.p_init(input_length)\n",
    "    torch.manual_seed(SEED)\n",
    "    loss, logits, hidden = model_forward(model, input_ids=input_ids, attention_mask=attention_mask, labels = label)\n",
    "    decode_loss, init_ids = prober.decode(hidden, origin_ids = input_ids)\n",
    "    \n",
    "    for i in init_ids:\n",
    "        if not i.tolist() in id_base:\n",
    "            id_base.append(i.tolist())\n",
    "            query += 1\n",
    "            adv_sentence = prober.attack(i, label)\n",
    "            if adv_sentence is not None:\n",
    "                use,I,ppl = cal_metric(ori_sentence, adv_sentence)\n",
    "                if use > USE_GATE:\n",
    "                    successed = True\n",
    "                    bpstep = -1\n",
    "                    successed = True\n",
    "                    return{\n",
    "                        'successed': successed,\n",
    "                        'query': query,\n",
    "                        'pll': pll,\n",
    "                        'I': I,\n",
    "                        'use': use,\n",
    "                        'bpstep': bpstep,\n",
    "                        'token_acc': rec_token_acc,\n",
    "                        'ori_sentence': ori_sentence,\n",
    "                        'adv_sentence': adv_sentence\n",
    "                    }\n",
    "\n",
    "\n",
    "\n",
    "    seg_step = 0\n",
    "    max_use = 0\n",
    "    \n",
    "    for seg_step in range(SEG_STEP):\n",
    "        lr = adv_lr\n",
    "        import gc\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        for i in tqdm(range(adv_steps)):\n",
    "\n",
    "            projected = p_layer.p_accu(loss * LOSS_WEIGHT - decode_loss * DECODE_WEIGHT, lr, input_length = input_length)\n",
    "\n",
    "            torch.manual_seed(SEED)\n",
    "            loss, logits, hidden =  model_forward(model, input_ids=input_ids, attention_mask=attention_mask, labels = label)\n",
    "            decode_loss, p_ids = prober.decode(hidden, origin_ids = input_ids)\n",
    "\n",
    "            for p_id in p_ids:\n",
    "                if not p_id.tolist() in id_base:\n",
    "                    id_base.append(p_id.tolist())\n",
    "                    query += 1\n",
    "                    adv_sentence = prober.attack(p_id, label)\n",
    "\n",
    "                    if adv_sentence is not None:\n",
    "                        use,I,ppl = cal_metric(ori_sentence, adv_sentence)\n",
    "#                         print(adv_sentence, \" USE = \",use)\n",
    "                        if use> max_use:\n",
    "                            max_use = use\n",
    "                        if use > USE_GATE:\n",
    "                            successed = True\n",
    "                            bpstep = seg_step * adv_steps + i\n",
    "                            successed = True\n",
    "                            return{\n",
    "                                'successed': successed,\n",
    "                                'query': query,\n",
    "                                'pll': pll,\n",
    "                                'I': I,\n",
    "                                'use': use,\n",
    "                                'bpstep': bpstep,\n",
    "                                'token_acc': rec_token_acc,\n",
    "                                'ori_sentence': ori_sentence,\n",
    "                                'adv_sentence': adv_sentence\n",
    "                            }\n",
    "#         print(\"################RESET!####################\", \" D_LOSS = \", (loss-origin_loss).norm(), \"BASED: \", len(id_base), \"Query; \", query)\n",
    "        input_ids = random_cover(ori_input_ids.clone())\n",
    "#         print(input_ids.tolist())\n",
    "        torch.manual_seed(SEED + seg_step)\n",
    "        p_layer.p_init(input_length)\n",
    "        loss, logits, hidden = model_forward(model, input_ids=input_ids, attention_mask=attention_mask, labels = label)\n",
    "    torch.cuda.empty_cache()\n",
    "    return{\n",
    "        'successed': successed,\n",
    "        'query': query,\n",
    "        'pll': pll,\n",
    "        'I': I,\n",
    "        'use': max_use,\n",
    "        'bpstep': bpstep,\n",
    "        'token_acc': rec_token_acc,\n",
    "        'ori_sentence': ori_sentence,\n",
    "        'adv_sentence': adv_sentence\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mistgpu/site-packages/datasets/formatting/formatting.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "  3%|▎         | 3/100 [00:20<11:10,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'successed': True, 'query': 5, 'pll': 0, 'I': 0, 'use': 0.9371679, 'bpstep': 3, 'token_acc': 0.896484375, 'ori_sentence': \"At the end of my review of Cache, I wrote that I was intrigued with Haneke as a film maker. This is what led me to get the DVD for La Pianiste, which I just finished watching about a half hour ago.<br /><br />It's all been expressed, here at IMDb and in many of the external reviews - the gruesomely twisted pathology that would 'create' an individual like Huppert's Erika, who is still trying, after years and years, to please her mother, at the expense of everyone and everything else in her life, beginning with her self. She's repressed everything that would free her from her self-imposed bondage, including, of course, her sexuality, which has literally imploded, to the point of madness, to where she can no longer even begin to comprehend what a genuine loving impulse would feel like.<br /><br />This is a graphic portrait of a severe emotional cripple, one who never found the strength to get out of her childhood situation and become a functioning adult. I think this subject relates to all of us - we're all striving for autonomy, but there are needs, so many conflicting needs, most of which are not even on the conscious level. It also deals brilliantly with the contrast between what one fantasizes about, sexually, and the reality of those fantasies, as well as the consequences of choosing to share one's sexual fantasies with another human being. Huppert's character gets what she asks for in the course of the film, and it is hardly the emancipating experience she had imagined it to be. <br /><br />Regarding the much-discussed scene in the bathroom: I really appreciated how this sequence had all the possible erotic charge (for the viewer, I mean) sucked out of it because of the prior scene, where she put the glass in the girl's pocket. By the time she's acting out her let's-see-if-this-guy-is-worthy scenario in the bathroom, we've already found out that she's dangerously disturbed and so it's not a turn on, her little domination session with our poor unsuspecting dupe.<br /><br />I think another incredible achievement of this movie is how, about halfway through it, I completely forgot that it was not in English and that I was reading sub-titles. That has never happened before, in any foreign movie, and I've seen quite a few. <br /><br />In this film, like Cache, the ending is not all wrapped up in a nice little tidy bow, but unlike Cache, we do at least get some sense of finality, despite the fact that we do not even know for sure whether Huppert's character is alive or dead. After experiencing La Pianiste, when it comes to Michael Haneke, I am, needless to say, more than a trifle intrigued.\", 'adv_sentence': \"at the end of my review of domesday, i wrote that i was intrigued with haneke as a film maker. this is what led me to hear the dvd for la pianiste, which i just finished watching about a half hour ago./br //br /?' pest've all been expressed, here at imdb and in many of the external reviews - the gruesomely twisted pathology that would 'cre out' an individual like huppert's erikawer who is still trying, after years and years, to please her mother, at the expense of happiness and everything else in her life, beginning with her self. she's repressed everything that would free her from her self-imposed bondage, we, of course, her sexuality, which has literally imploded, to the point of madness, to where she cannot cannot longer even begin to comprehend what a genuine loving impulse would feel like./br //br /?'this is a graphic portrait of a severe emotional failure, one who never found the strength to get out of her childhood situation and become a functioning adult. i think this subject relates to all of us;- we're all striving for autonomy, but there are needs, so many conflicting needs, most of which are not even on the conscious level. it also deals brilliantly with the difference between what one fantasizes aboutalys sexually, and the reality of those fantasies henning as well as the consequences of choosing to share one's sexual fantasies with another human being. huppert's character gets what she asks for in second course of the film, and it is hardly the emancipating experience she had imagined it to be. /br //br /?'regard destroying the much norwegiandiscusces several scene in the bathroom: i really appreciated how this sequence had all the possible erotic assistance ( for the viewer, i mean) sucked out of it because of the prior scene henning where she put the glass in the girl's pocket. by the time she'll acting out her letting'skhovsee- if-this-guy-is-worthy scenario in the bathroom, we've already bing out that she's dangerously disturbed and so it' is not a turn on, her little laughing session with our poor unsuspectling duptch./br //br /?'final think another!! achievement of this movie is how, about halfway through it, i freaking forgot that louder\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "import seaborn as sb\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "SEG_STEP = 10\n",
    "\n",
    "HIDDEN_INDEX = 11\n",
    "INIT_MAG = 1\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "USE_GATE = 0.87\n",
    "\n",
    "SEED = 114514\n",
    "\n",
    "\n",
    "LOSS_WEIGHT = 1\n",
    "DECODE_WEIGHT = 0.01\n",
    "key = 'test'\n",
    "\n",
    "def exp_lr(adv_lr, step):\n",
    "    if step > 200:\n",
    "        adv_lr -= 1/400\n",
    "    return adv_lr\n",
    "\n",
    "suca = []\n",
    "alla = []\n",
    "import random\n",
    "random.seed(SEED)\n",
    "for i in (range(698,699)):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    ret = attack_step(\n",
    "        dataset = dataset, \n",
    "        encoded_dataset = encoded_dataset, \n",
    "        index = i, \n",
    "        adv_lr = 7,\n",
    "        adv_steps = 100, \n",
    "        start_layer = 0, \n",
    "        prober = prober,\n",
    "        scheduler = None,\n",
    "        SEED = SEED\n",
    "        )\n",
    "    print(ret)\n",
    "    if ret is not None:\n",
    "        if ret['successed']:\n",
    "            suca.append(ret)\n",
    "        alla.append(ret)\n",
    "    if i%5 ==0:\n",
    "        torch.save(alla, \"albert_alla_696_\" + str(i) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1da6e8aecb0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.zero_grad(set_to_none = True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sb\n",
    "stat = {\n",
    "        'successed': [],\n",
    "        'query': [],\n",
    "        'pll': [],\n",
    "        'I': [],\n",
    "        'use': [],\n",
    "        'bpstep': [],\n",
    "        'token_acc': [],\n",
    "    }\n",
    "for key in stat.keys():\n",
    "    for data in alla:\n",
    "        stat[key].append(data[key])\n",
    "    sb.displot(stat[key])\n",
    "    if key != 'successed':\n",
    "        stat[key] = np.array(stat[key]).mean()\n",
    "    else:\n",
    "        stat[key] = len(suca)/len(alla)\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = []\n",
    "bar = 0\n",
    "alla = torch.load('alla2.pt')\n",
    "for data in alla:\n",
    "    u.append(data['use'])\n",
    "u.sort()\n",
    "u = np.array(u)\n",
    "print(u[bar:].mean())\n",
    "print(len(u[bar:]) / len(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(suca)/len(alla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in alla:\n",
    "    if data['use'] > 0.85:\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100/102"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
